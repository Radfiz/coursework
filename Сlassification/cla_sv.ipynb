{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddde29b9-e3eb-4c5c-8dd4-d127058ba0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "from scipy.stats import uniform, randint\n",
    "import optuna\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Загрузка данных ---\n",
    "file_url_pca = 'https://drive.google.com/uc?export=download&id=1SuUhkpfj-3uJQnxwmUCyDUogfa2TixTe'\n",
    "file_url_manual = 'https://drive.google.com/uc?export=download&id=1p8VYp23oOylSFrfJztQVheNLop-bX40o'\n",
    "\n",
    "df_pca = pd.read_csv(file_url_pca, encoding='utf-8')\n",
    "df_manual = pd.read_csv(file_url_manual, encoding='utf-8')\n",
    "\n",
    "# --- Определяем фактические целевые переменные, которые уже логарифмированы ---\n",
    "# Поскольку вы подтвердили, что 'IC50, mM', 'CC50, mM', 'SI' уже логарифмированы,\n",
    "# мы будем использовать их напрямую как наши \"лог-цели\".\n",
    "TARGETS_ACTUAL_LOGGED = ['IC50, mM', 'CC50, mM', 'SI']\n",
    "\n",
    "print(\"PCA Data (actual logged targets):\")\n",
    "print(df_pca[TARGETS_ACTUAL_LOGGED].head())\n",
    "print(\"\\nManual Data (actual logged targets):\")\n",
    "print(df_manual[TARGETS_ACTUAL_LOGGED].head())\n",
    "\n",
    "# --- Создание бинарных целевых переменных для классификации ---\n",
    "\n",
    "classification_targets = {}\n",
    "\n",
    "# 1. IC50 > медианы\n",
    "median_ic50_pca = df_pca['IC50, mM'].median()\n",
    "df_pca['is_IC50_above_median'] = (df_pca['IC50, mM'] > median_ic50_pca).astype(int)\n",
    "median_ic50_manual = df_manual['IC50, mM'].median()\n",
    "df_manual['is_IC50_above_median'] = (df_manual['IC50, mM'] > median_ic50_manual).astype(int)\n",
    "classification_targets['is_IC50_above_median'] = 'IC50, mM' # Указываем, откуда взята целевая\n",
    "\n",
    "# 2. CC50 > медианы\n",
    "median_cc50_pca = df_pca['CC50, mM'].median()\n",
    "df_pca['is_CC50_above_median'] = (df_pca['CC50, mM'] > median_cc50_pca).astype(int)\n",
    "median_cc50_manual = df_manual['CC50, mM'].median()\n",
    "df_manual['is_CC50_above_median'] = (df_manual['CC50, mM'] > median_cc50_manual).astype(int)\n",
    "classification_targets['is_CC50_above_median'] = 'CC50, mM'\n",
    "\n",
    "# 3. SI > медианы\n",
    "median_si_pca = df_pca['SI'].median()\n",
    "df_pca['is_SI_above_median'] = (df_pca['SI'] > median_si_pca).astype(int)\n",
    "median_si_manual = df_manual['SI'].median()\n",
    "df_manual['is_SI_above_median'] = (df_manual['SI'] > median_si_manual).astype(int)\n",
    "classification_targets['is_SI_above_median'] = 'SI'\n",
    "\n",
    "# 4. SI > 8 (поскольку SI уже логарифмировано, порог 8 должен быть логарифмирован)\n",
    "# Если SI было получено как np.log1p(SI_original), то порог тоже должен быть np.log1p(8)\n",
    "log_8_threshold = np.log1p(8) # Предполагаем, что исходное SI было логарифмировано с log1p\n",
    "df_pca['is_SI_above_8'] = (df_pca['SI'] > log_8_threshold).astype(int)\n",
    "df_manual['is_SI_above_8'] = (df_manual['SI'] > log_8_threshold).astype(int)\n",
    "classification_targets['is_SI_above_8'] = 'SI'\n",
    "\n",
    "print(\"\\nСозданные бинарные целевые переменные:\")\n",
    "print(\"PCA - is_IC50_above_median value counts:\\n\", df_pca['is_IC50_above_median'].value_counts())\n",
    "print(\"PCA - is_CC50_above_median value counts:\\n\", df_pca['is_CC50_above_median'].value_counts())\n",
    "print(\"PCA - is_SI_above_median value counts:\\n\", df_pca['is_SI_above_median'].value_counts())\n",
    "print(\"PCA - is_SI_above_8 value counts:\\n\", df_pca['is_SI_above_8'].value_counts())\n",
    "\n",
    "print(\"\\nManual - is_IC50_above_median value counts:\\n\", df_manual['is_IC50_above_median'].value_counts())\n",
    "print(\"Manual - is_CC50_above_median value counts:\\n\", df_manual['is_CC50_above_median'].value_counts())\n",
    "print(\"Manual - is_SI_above_median value counts:\\n\", df_manual['is_SI_above_median'].value_counts())\n",
    "print(\"Manual - is_SI_above_8 value counts:\\n\", df_manual['is_SI_above_8'].value_counts())\n",
    "\n",
    "# --- Вспомогательная функция для расчета метрик классификации ---\n",
    "def calculate_classification_metrics(y_true, y_pred, y_pred_proba):\n",
    "    \"\"\"Вычисляет метрики классификации: Accuracy, Precision, Recall, F1, ROC-AUC.\"\"\"\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, zero_division=0) # Добавлено zero_division\n",
    "    recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "    roc_auc = roc_auc_score(y_true, y_pred_proba)\n",
    "    return accuracy, precision, recall, f1, roc_auc\n",
    "\n",
    "# --- Функции для каждого метода оптимизации (адаптированные для классификации) ---\n",
    "\n",
    "def run_randomized_search_classifier(model_instance, param_distributions, X_train_scaled, y_train, n_iter_search=20):\n",
    "    \"\"\"Выполняет RandomizedSearchCV для подбора гиперпараметров для классификации.\"\"\"\n",
    "    if not param_distributions:\n",
    "        model_instance.fit(X_train_scaled, y_train)\n",
    "        return model_instance, {}\n",
    "\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    random_search = RandomizedSearchCV(model_instance, param_distributions, n_iter=n_iter_search,\n",
    "                                       cv=cv, scoring='roc_auc',\n",
    "                                       n_jobs=-1, verbose=0, random_state=42)\n",
    "    random_search.fit(X_train_scaled, y_train)\n",
    "    return random_search.best_estimator_, random_search.best_params_\n",
    "\n",
    "def run_grid_search_classifier(model_instance, param_grid, X_train_scaled, y_train):\n",
    "    \"\"\"Выполняет GridSearchCV для подбора гиперпараметров для классификации.\"\"\"\n",
    "    if not param_grid:\n",
    "        model_instance.fit(X_train_scaled, y_train)\n",
    "        return model_instance, {}\n",
    "\n",
    "    cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    grid_search = GridSearchCV(model_instance, param_grid, cv=cv, scoring='roc_auc',\n",
    "                               n_jobs=-1, verbose=0)\n",
    "    grid_search.fit(X_train_scaled, y_train)\n",
    "    return grid_search.best_estimator_, grid_search.best_params_\n",
    "\n",
    "def run_optuna_search_classifier(model_class, optuna_search_space, X_train_scaled, y_train, n_trials=20):\n",
    "    \"\"\"Выполняет оптимизацию гиперпараметров с помощью Optuna для классификации.\"\"\"\n",
    "    def objective(trial):\n",
    "        params = optuna_search_space(trial)\n",
    "\n",
    "        # Обработка random_state/random_seed для Optuna\n",
    "        # Random_state может быть не поддерживаем для всех моделей или определенных solvers\n",
    "        # Здесь мы исходим из того, что Optuna space уже определяет правильный параметр ('random_state' или 'random_seed')\n",
    "        if model_class in [LogisticRegression, MLPClassifier] and 'random_state' in params:\n",
    "            model = model_class(**{k: v for k, v in params.items() if k != 'random_state'})\n",
    "        else:\n",
    "            model = model_class(**params)\n",
    "\n",
    "        kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        cv_scores = []\n",
    "        for train_idx, val_idx in kf.split(X_train_scaled, y_train):\n",
    "            X_train_fold, X_val_fold = X_train_scaled[train_idx], X_train_scaled[val_idx]\n",
    "            y_train_fold, y_val_fold = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "\n",
    "            try:\n",
    "                if isinstance(model, CatBoostClassifier):\n",
    "                    train_pool = Pool(X_train_fold, y_train_fold)\n",
    "                    val_pool = Pool(X_val_fold, y_val_fold)\n",
    "                    model.fit(train_pool, eval_set=val_pool, early_stopping_rounds=10, verbose=False)\n",
    "                    y_val_pred_proba = model.predict_proba(X_val_fold)[:, 1]\n",
    "                else:\n",
    "                    model.fit(X_train_fold, y_train_fold)\n",
    "                    y_val_pred_proba = model.predict_proba(X_val_fold)[:, 1]\n",
    "\n",
    "                roc_auc_fold = roc_auc_score(y_val_fold, y_val_pred_proba)\n",
    "                cv_scores.append(roc_auc_fold)\n",
    "            except Exception as e:\n",
    "                # print(f\"Ошибка при обучении/предсказании в Optuna (фолд): {e}\") # Для дебага\n",
    "                return -float('inf')\n",
    "\n",
    "        return -np.mean(cv_scores)\n",
    "\n",
    "    study = optuna.create_study(direction='minimize', sampler=optuna.samplers.TPESampler(seed=42))\n",
    "    study.optimize(objective, n_trials=n_trials, show_progress_bar=False, catch=(ValueError, Exception))\n",
    "\n",
    "    best_params = study.best_params\n",
    "    \n",
    "    # Final model instance with best parameters\n",
    "    if model_class in [LogisticRegression, MLPClassifier] and 'random_state' in best_params:\n",
    "        best_model_instance = model_class(**{k: v for k, v in best_params.items() if k != 'random_state'})\n",
    "    else:\n",
    "        best_model_instance = model_class(**best_params)\n",
    "\n",
    "    try:\n",
    "        if isinstance(best_model_instance, CatBoostClassifier):\n",
    "            train_pool_final = Pool(X_train_scaled, y_train)\n",
    "            best_model_instance.fit(train_pool_final, verbose=False)\n",
    "        else:\n",
    "            best_model_instance.fit(X_train_scaled, y_train)\n",
    "    except Exception as e:\n",
    "        # print(f\"Ошибка при окончательном обучении CatBoost: {e}\") # Для дебага\n",
    "        return None, {}\n",
    "\n",
    "    return best_model_instance, best_params\n",
    "\n",
    "# --- Общая функция для оценки моделей с различными оптимизаторами (адаптированная) ---\n",
    "def evaluate_model_with_optimizer_classifier(model_name, model_class, params_config, X, y, target_name, optimizer_type):\n",
    "    \"\"\"Оценивает производительность модели классификации, используя указанный метод оптимизации.\"\"\"\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    best_model = None\n",
    "    best_params = {}\n",
    "\n",
    "    # Инициализация параметров для воспроизводимости:\n",
    "    model_init_params = {}\n",
    "    if model_name == \"CatBoostClassifier\":\n",
    "        model_init_params['random_seed'] = 42\n",
    "    elif model_name in [\"LogisticRegression\", \"RandomForestClassifier\", \"XGBClassifier\", \"MLPClassifier\"]:\n",
    "        # Эти модели обычно принимают random_state для воспроизводимости\n",
    "        model_init_params['random_state'] = 42\n",
    "\n",
    "    if optimizer_type == 'RandomizedSearchCV':\n",
    "        param_distributions = params_config.get('random_dist', {})\n",
    "        # Для LogisticRegression, если нет dist, используем дефолтный инстанс\n",
    "        if model_name == \"LogisticRegression\" and not param_distributions:\n",
    "             model_instance = model_class(**model_init_params)\n",
    "             model_instance.fit(X_train_scaled, y_train)\n",
    "             best_model, best_params = model_instance, {}\n",
    "        else:\n",
    "            best_model, best_params = run_randomized_search_classifier(model_class(**model_init_params), param_distributions, X_train_scaled, y_train, n_iter_search=20)\n",
    "\n",
    "    elif optimizer_type == 'GridSearchCV':\n",
    "        param_grid = params_config.get('grid_params', {})\n",
    "        if not param_grid:\n",
    "            model_instance = model_class(**model_init_params)\n",
    "            model_instance.fit(X_train_scaled, y_train)\n",
    "            best_model, best_params = model_instance, {}\n",
    "        else:\n",
    "            best_model, best_params = run_grid_search_classifier(model_class(**model_init_params), param_grid, X_train_scaled, y_train)\n",
    "\n",
    "    elif optimizer_type == 'Optuna':\n",
    "        optuna_space = params_config.get('optuna_space')\n",
    "        if optuna_space is None:\n",
    "            model_instance = model_class(**model_init_params)\n",
    "            model_instance.fit(X_train_scaled, y_train)\n",
    "            best_model, best_params = model_instance, {}\n",
    "        else:\n",
    "            # Optuna уже обрабатывает random_state/random_seed в своей objective функции\n",
    "            best_model, best_params = run_optuna_search_classifier(model_class, optuna_space, X_train_scaled, y_train, n_trials=20)\n",
    "    else:\n",
    "        raise ValueError(f\"Неизвестный тип оптимизатора: {optimizer_type}\")\n",
    "\n",
    "    if best_model is None:\n",
    "        return None\n",
    "\n",
    "    y_pred = best_model.predict(X_test_scaled)\n",
    "    # predict_proba может отсутствовать для некоторых моделей (например, SVM с probability=False)\n",
    "    # или если модель не была обучена с этой функциональностью.\n",
    "    # Проверяем наличие predict_proba\n",
    "    if hasattr(best_model, \"predict_proba\") and len(best_model.predict_proba(X_test_scaled).shape) > 1:\n",
    "        y_pred_proba = best_model.predict_proba(X_test_scaled)[:, 1]\n",
    "    else:\n",
    "        # Для моделей без predict_proba, ROC-AUC не может быть рассчитан.\n",
    "        # В таком случае, можно либо пропустить ROC-AUC, либо вернуть NaN.\n",
    "        # Для SVM, если probability=True не установлен при инициализации, его не будет.\n",
    "        # Для LogisticRegression и Tree-based моделей predict_proba всегда есть.\n",
    "        print(f\"Warning: Model {model_name} does not have predict_proba or it's not applicable. ROC-AUC will be NaN.\")\n",
    "        y_pred_proba = np.full_like(y_pred, np.nan, dtype=float) # Заполняем NaN для ROC-AUC\n",
    "\n",
    "    accuracy, precision, recall, f1, roc_auc = calculate_classification_metrics(y_test, y_pred, y_pred_proba)\n",
    "\n",
    "    return {\n",
    "        'model': model_name,\n",
    "        'optimizer': optimizer_type,\n",
    "        'target': target_name,\n",
    "        'best_params': best_params,\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'roc_auc': roc_auc\n",
    "    }\n",
    "\n",
    "# --- Определение моделей и их гиперпараметров для разных оптимизаторов (адаптированные для классификации) ---\n",
    "models_config_classifier = {\n",
    "    \"LogisticRegression\": {\n",
    "        \"class\": LogisticRegression,\n",
    "        \"random_dist\": {\n",
    "            'C': uniform(loc=0.1, scale=10),\n",
    "            'penalty': ['l1', 'l2'],\n",
    "            'solver': ['liblinear']\n",
    "        },\n",
    "        \"grid_params\": {\n",
    "            'C': [0.1, 1.0, 10.0],\n",
    "            'penalty': ['l1', 'l2'],\n",
    "            'solver': ['liblinear']\n",
    "        },\n",
    "        \"optuna_space\": lambda trial: {\n",
    "            'C': trial.suggest_float('C', 0.1, 10.0, log=True),\n",
    "            'penalty': trial.suggest_categorical('penalty', ['l1', 'l2']),\n",
    "            'solver': 'liblinear',\n",
    "            'random_state': 42 # Добавлен random_state здесь, чтобы управлять им\n",
    "        }\n",
    "    },\n",
    "    \"RandomForestClassifier\": {\n",
    "        \"class\": RandomForestClassifier,\n",
    "        \"random_dist\": {\n",
    "            'n_estimators': randint(50, 200),\n",
    "            'max_depth': [5, 10, None],\n",
    "            'min_samples_split': randint(2, 8)\n",
    "        },\n",
    "        \"grid_params\": {\n",
    "            'n_estimators': [100, 150],\n",
    "            'max_depth': [5, 10],\n",
    "        },\n",
    "        \"optuna_space\": lambda trial: {\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 50, 200),\n",
    "            'max_depth': trial.suggest_categorical('max_depth', [5, 10, 15, None]),\n",
    "            'min_samples_split': trial.suggest_int('min_samples_split', 2, 8),\n",
    "            'random_state': 42\n",
    "        }\n",
    "    },\n",
    "    \"XGBClassifier\": {\n",
    "        \"class\": XGBClassifier,\n",
    "        \"random_dist\": {\n",
    "            'n_estimators': randint(50, 200),\n",
    "            'learning_rate': uniform(0.01, 0.15),\n",
    "            'max_depth': randint(3, 8),\n",
    "            'subsample': uniform(0.7, 0.3),\n",
    "            'use_label_encoder': [False]\n",
    "        },\n",
    "        \"grid_params\": {\n",
    "            'n_estimators': [100, 150],\n",
    "            'learning_rate': [0.05, 0.1],\n",
    "            'max_depth': [3, 5],\n",
    "            'use_label_encoder': [False]\n",
    "        },\n",
    "        \"optuna_space\": lambda trial: {\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 50, 200),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.15, log=True),\n",
    "            'max_depth': trial.suggest_int('max_depth', 3, 8),\n",
    "            'subsample': trial.suggest_float('subsample', 0.7, 1.0),\n",
    "            'eval_metric': 'logloss',\n",
    "            'n_jobs': -1,\n",
    "            'random_state': 42,\n",
    "            'use_label_encoder': False\n",
    "        }\n",
    "    },\n",
    "    \"CatBoostClassifier\": {\n",
    "        \"class\": CatBoostClassifier,\n",
    "        \"random_dist\": {\n",
    "            'iterations': randint(50, 200),\n",
    "            'learning_rate': uniform(0.01, 0.15),\n",
    "            'depth': randint(3, 8),\n",
    "            'l2_leaf_reg': uniform(1, 7)\n",
    "        },\n",
    "        \"grid_params\": {\n",
    "            'iterations': [100, 150],\n",
    "            'learning_rate': [0.05, 0.1],\n",
    "            'depth': [3, 5]\n",
    "        },\n",
    "        \"optuna_space\": lambda trial: {\n",
    "            'iterations': trial.suggest_int('iterations', 50, 200),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.15, log=True),\n",
    "            'depth': trial.suggest_int('depth', 3, 8),\n",
    "            'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1e-2, 10, log=True),\n",
    "            'verbose': False,\n",
    "            'random_seed': 42, # CatBoost uses random_seed\n",
    "            'thread_count': -1,\n",
    "            'objective': 'Logloss'\n",
    "        }\n",
    "    },\n",
    "    \"MLPClassifier\": {\n",
    "        \"class\": MLPClassifier,\n",
    "        \"random_dist\": {\n",
    "            'hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 50)],\n",
    "            'alpha': uniform(0.0001, 0.005),\n",
    "            'learning_rate_init': uniform(0.0001, 0.005)\n",
    "        },\n",
    "        \"grid_params\": {\n",
    "            'hidden_layer_sizes': [(50,), (100,)],\n",
    "            'alpha': [0.0001, 0.001]\n",
    "        },\n",
    "        \"optuna_space\": lambda trial: {\n",
    "            'hidden_layer_sizes': trial.suggest_categorical('hidden_layer_sizes', [(50,), (100,), (50, 50), (100, 50)]),\n",
    "            'alpha': trial.suggest_float('alpha', 1e-5, 1e-2, log=True),\n",
    "            'learning_rate_init': trial.suggest_float('learning_rate_init', 1e-4, 1e-2, log=True),\n",
    "            'max_iter': 2000,\n",
    "            'random_state': 42,\n",
    "            'solver': 'adam'\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65441089-d139-4982-9de4-11c9d5367183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Основной цикл оценки с тремя методами оптимизации ---\n",
    "all_classification_results = []\n",
    "optimizers = ['RandomizedSearchCV', 'GridSearchCV', 'Optuna']\n",
    "\n",
    "# Извлекаем признаки, исключая все целевые переменные (теперь просто TARGETS_ACTUAL_LOGGED)\n",
    "# и новые бинарные целевые переменные.\n",
    "columns_to_drop_common = TARGETS_ACTUAL_LOGGED + list(classification_targets.keys())\n",
    "\n",
    "# Добавляем специфические для датасетов столбцы, которые не являются признаками (например, SMILES)\n",
    "if 'SMILES' in df_pca.columns:\n",
    "    columns_to_drop_pca_final = columns_to_drop_common + ['SMILES']\n",
    "else:\n",
    "    columns_to_drop_pca_final = columns_to_drop_common\n",
    "\n",
    "if 'SMILES' in df_manual.columns:\n",
    "    columns_to_drop_manual_final = columns_to_drop_common + ['SMILES']\n",
    "else:\n",
    "    columns_to_drop_manual_final = columns_to_drop_common\n",
    "\n",
    "X_pca_features = df_pca.drop(columns=columns_to_drop_pca_final, errors='ignore')\n",
    "X_manual_features = df_manual.drop(columns=columns_to_drop_manual_final, errors='ignore')\n",
    "\n",
    "\n",
    "print(\"Начинаем процесс обучения и оценки моделей классификации...\")\n",
    "\n",
    "for target_name_classification in tqdm(classification_targets.keys(), desc=\"Прогнозирование задач классификации\"):\n",
    "    for data_source_name, X_data_features, df_data in [(\"PCA Aggregated\", X_pca_features, df_pca), (\"Manual Aggregated\", X_manual_features, df_manual)]:\n",
    "        y_data_classification = df_data[target_name_classification]\n",
    "\n",
    "        num_models_to_run = 0\n",
    "        for model_name, config in models_config_classifier.items():\n",
    "            for optimizer_type in optimizers:\n",
    "                # Уточненная логика для подсчета моделей\n",
    "                if (optimizer_type == 'RandomizedSearchCV' and not config.get('random_dist', {})) and model_name != \"LogisticRegression\":\n",
    "                    continue\n",
    "                if (optimizer_type == 'GridSearchCV' and not config.get('grid_params', {})) and model_name != \"LogisticRegression\":\n",
    "                    continue\n",
    "                if (optimizer_type == 'Optuna' and config.get('optuna_space') is None) and model_name != \"LogisticRegression\":\n",
    "                    continue\n",
    "                num_models_to_run += 1\n",
    "\n",
    "        with tqdm(total=num_models_to_run, desc=f\"Оптимизация для {target_name_classification} ({data_source_name})\", leave=False) as pbar_inner:\n",
    "            for optimizer_type in optimizers:\n",
    "                for model_name, config in models_config_classifier.items():\n",
    "                    # Пропускаем неподходящие комбинации модель-оптимизатор, чтобы избежать ошибок и не тратить время\n",
    "                    if (optimizer_type == 'RandomizedSearchCV' and not config.get('random_dist', {})) and model_name != \"LogisticRegression\":\n",
    "                        pbar_inner.update(1)\n",
    "                        continue\n",
    "                    if (optimizer_type == 'GridSearchCV' and not config.get('grid_params', {})) and model_name != \"LogisticRegression\":\n",
    "                        pbar_inner.update(1)\n",
    "                        continue\n",
    "                    if (optimizer_type == 'Optuna' and config.get('optuna_space') is None) and model_name != \"LogisticRegression\":\n",
    "                        pbar_inner.update(1)\n",
    "                        continue\n",
    "\n",
    "                    model_class = config[\"class\"]\n",
    "                    params_config = config\n",
    "\n",
    "                    pbar_inner.set_description(f\"Оптимизация для {target_name_classification} ({data_source_name}) - {model_name} ({optimizer_type})\")\n",
    "\n",
    "                    result = evaluate_model_with_optimizer_classifier(model_name, model_class, params_config,\n",
    "                                                                      X_data_features, y_data_classification, target_name_classification, optimizer_type)\n",
    "                    if result:\n",
    "                        result['data_source'] = data_source_name\n",
    "                        all_classification_results.append(result)\n",
    "                    pbar_inner.update(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4fab20-7400-4d47-86f9-ccb611faebdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Сохранение и вывод результатов ---\n",
    "output_classification_file = Path('classification_results_all_optimizers_50_iter.csv')\n",
    "\n",
    "all_classification_results_df = pd.DataFrame(all_classification_results)\n",
    "all_classification_results_df.to_csv(output_classification_file, index=False)\n",
    "print(f\"\\nРезультаты классификации сохранены в: {output_classification_file}\")\n",
    "\n",
    "print(\"\\n--- Сводка результатов классификации по методам оптимизации ---\")\n",
    "\n",
    "for optimizer in optimizers:\n",
    "    print(f\"\\n## Результаты {optimizer} (Классификация):\")\n",
    "    subset_optimizer = all_classification_results_df[all_classification_results_df['optimizer'] == optimizer]\n",
    "    print(subset_optimizer.sort_values(by=['target', 'roc_auc'], ascending=[True, False]).to_string())\n",
    "    print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "\n",
    "# Визуализация метрик классификации \n",
    "classification_metrics_to_plot = ['roc_auc', 'f1_score', 'accuracy']\n",
    "\n",
    "for target_class in classification_targets.keys():\n",
    "    for metric in classification_metrics_to_plot:\n",
    "        plt.figure(figsize=(16, 8))\n",
    "        subset = all_classification_results_df[all_classification_results_df['target'] == target_class].sort_values(by=metric, ascending=False)\n",
    "        sns.barplot(x='model', y=metric, hue='optimizer', data=subset, palette='viridis')\n",
    "        plt.title(f'Сравнение {metric.upper()} для \"{target_class}\" по методам оптимизации', fontsize=16)\n",
    "        plt.ylabel(metric.upper(), fontsize=12)\n",
    "        plt.xlabel('Модель', fontsize=12)\n",
    "        plt.xticks(rotation=45, ha='right', fontsize=10)\n",
    "        plt.yticks(fontsize=10)\n",
    "        plt.legend(title='Метод оптимизации', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'classification_{target_class}_{metric}_comparison.png')\n",
    "        plt.close() # Close plot to free memory"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
